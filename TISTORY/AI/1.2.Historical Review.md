지금까지 어떤 연구들이 중요했고, 트렌드인지 살펴보자.

2012 – AlexNet : 딥러닝의 역사적 시작. 이미지가 들어갔을 때 분류하는 것이 목적. 딥러닝의 성능을 보여줌
![Image](https://i.imgur.com/zHLziIj.png)

2013 – DQN : 딥마인드, 알파고 등의 기반
![Image](https://i.imgur.com/QUQvnfv.png)

2014 – Encoder/Decoder : 단어가 주어졌을 때 시퀀스를 만들 때 활용
![Image](https://i.imgur.com/Rq3Wi8d.png)

2014 – Adam Optimizer : 실험결과가 웬만하면 잘나온다
![Image](https://i.imgur.com/eut6uPv.png)

2015 – Generative Adversarial Network : 네트워크가 생성자(Generator) / 구분자 (Discriminator) 를 만들어서 학습
![Image](https://i.imgur.com/0R8JvUw.png)

~~술 마시는 게 연구에 좋다는 사례~~

2015 – Residual Networks : Network를 깊게 쌓을 수 있게 만들어줌 (원래는 성능이 안 나와서 깊게 쌓을 수가 없었음.)

![Image](https://i.imgur.com/MksABoY.png)

2017 – Transformer : recurrent Layer Network에 비한 장점을 살펴 볼 것.

![Image](https://i.imgur.com/MTOtXEV.png)

2018 – BERT : 방대한 말뭉치를 활용하여 Pre-training 한 후, 풀고자 하는 문제의 소스 데이터에 이 네트워크를 Fine – Tuning 해 좋은 성능을 낸다.

![Image](https://i.imgur.com/m8GEQDy.png)

2019 – Big Language Models(GPT-X) : 약간의 파인 튜닝으로 다양한 표/시퀀스 모델 등을 만들 수 있으며, 많은 파라미터로 이루어진 모델.

![Image](https://i.imgur.com/PuEdcLS.png)

2020 – Self Supervised Learning : SimCLR, 한정된 학습데이터 외에 라벨을 모르는 데이터를 활용한다.

![Image](https://i.imgur.com/lzbW7jz.png)

- `해당 비지니스에 대해 도메인 지식이 많은 상태에서 내가 직접 데이터를 만들어서 뻥튀기 시키겠다.` 또한 Self Supervised Learning