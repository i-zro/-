### 선형회귀
- 가장 간단한 Linear Neural Networks 
- 미분을 통해 loss의 최소화를 구하는 모델의 최적화를 진행. Backpropagation

![Image](https://i.imgur.com/E80B73V.png)

- W와 b에 대해 반복적으로 update하여 최적의 w와 b를 구함. 단, update stepsize는 작게 설정해야 함.

- 하지만, 세상은 선형으로 이루어져 있지 않음.
- m차원에서 n차원으로 가고 싶다 => 행렬 사용하자!

![Image](https://i.imgur.com/knruOLz.png)

- stack을 더 쌓고싶어?

=> deep하게 쌓아가면 됨. 근데 단순히 이렇게 쌓으면 한단짜리 선형변환과 다를 게 없다!

![Image](https://i.imgur.com/qmhv7we.png)

## 스택을 의미있게 쌓으려면?
![Image](https://i.imgur.com/tJDei4e.png)

- 단순히 선형 결합을 랜덤하게 반복하는 게 아니라, nonlinearity가 있는 activation function이 필요
(ReLU, Sigmoid, Tanh) 모델마다 필요한 activation이 다 다름.

![Image](https://i.imgur.com/b4nbkWi.png)

- 더 깊게 여러 레이어를 쌓아 퍼셉트론 구조로 만들 수도 있다.
![Image](https://i.imgur.com/LtAZRHN.png)

## Loss Function
![Image](https://i.imgur.com/x4cyUrb.png)

- Regression Task : MSE = 0이라고 항상 최적의 모델인 것은 아님.
loss는 L-1 norm(절댓값), L-2 norm(제곱) 혹은 다른 것을 사용해도 무관
L-2는 outliar가 있을 때 영향을 많이 받기 때문에(robustness(이상치가 등장했을 때 loss function이 얼마나 영향을 받는지를 뜻하는 용어)가 낮다) 주의할 필요가 있음

- Classification : 크로스 엔트로피라고도 함. yid(i = index, d = class)는 one-hot vector라서 정답인 차원의 class만 1이고 나머지는 0
=> NN의 출력값 중에서, 해당하는 차원의 class의 값만 높이겠다(100이든 10만이든 얼마나 높아지는진 상관x)

- Probabilistic : 결과가 단순히 숫자가 아니라 확률적인 것을 맞추고 싶을 때. 20대 이긴 한데 모르겠어 또는 30대인데 확실해! 이런 걸 맞추고 싶을 때.